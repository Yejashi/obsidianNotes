# **Building Polynomial Regression Models Using Linear Algebra**

## Slide 1: Title Slide

- **Talk Title:** _Building Polynomial Regression Models Using Linear Algebra_
    
- **Presenter:** [Your Name], PhD Student – Global Computing Lab (Advisor: Dr. Michela Taufer)
    
- **Affiliation:** University of Tennessee, Knoxville (Dept. of Electrical Engineering & Computer Science)
    

**Speaker Notes:**  
Welcome to this talk on _“Building Polynomial Regression Models Using Linear Algebra.”_ My name is [Your Name], and I’m a PhD student in the Global Computing Laboratory at the University of Tennessee under Dr. Michela Taufer. Today I’ll be discussing how we construct polynomial regression models and why linear algebra is fundamental to this process. I’ll also highlight applications in High-Performance Computing (HPC), including results from our recent Surrogate-Based Modeling (SBM) study presented at ICCS 2025. By the end, I hope to convey both the theory behind polynomial regression using linear systems and its practical impact on modeling HPC application performance.

## Slide 2: Test Questions (Pre-Talk)

1. **Formulation:** How can fitting a polynomial be expressed as solving a linear system $A\mathbf{x} = \mathbf{b}$, and what linear algebra methods can solve it?
    
2. **HPC Motivation:** Why use surrogate models like polynomial regression for HPC performance tuning instead of exhaustive parameter searches?
    
3. **Model Complexity:** What is a potential risk of using a very high-degree polynomial in regression, and how can we mitigate it?
    

**Speaker Notes:**  
Before we dive in, consider these guiding questions. (1) _Formulation:_ How do we set up polynomial regression as a linear algebra problem $A x = b$, and which techniques (LU factorization, QR, SVD, etc.) help solve it efficiently? (2) _HPC Motivation:_ In the context of HPC, why might we prefer building a surrogate model (like a polynomial fit) to predict performance, rather than running exhaustive experiments for every configuration? (3) _Model Complexity:_ What happens if we make our polynomial model too complex (too high-degree) relative to the available data, and how do we guard against this issue (think overfitting and remedies like cross-validation)? Keep these questions in mind; we will answer them during the talk. We’ll revisit them at the end to check understanding.

## Slide 3: Presenter Introduction – Academic & Personal

- **Academic Background:** PhD Student in **Global Computing Lab** (GCLab) under Dr. Michela Taufer. Research focus on HPC performance analysis, surrogate modeling, and linear algebra applications.
    
- **Hometown:** [City, Country]. _(Include a map or image of hometown)_
    
- **Interests & Hobbies:** Passionate about coding and math; enjoy hiking in the Smoky Mountains and playing chess. Big fan of sci-fi movies and trying out new cuisines.
    
- **Fun Facts:** Proud owner of a mischievous cat (who occasionally tries to help me code). Have traveled to 10+ countries – love exploring new cultures and food.
    

**Speaker Notes:**  
Let me briefly introduce myself. I’m a second-year PhD student in the Global Computing Lab, which is led by Dr. Michela Taufer. My research revolves around **high performance computing** and how we can use mathematical modeling to understand and predict application performance. In particular, I’m interested in **surrogate-based modeling** – using simplified models like polynomial regressions to approximate complex system behaviors. I originally hail from [Your Hometown]; you can see it on the map here. Outside of the lab, I like to balance work with some fun: I go hiking (we have the Great Smoky Mountains near Knoxville, which are fantastic!), I enjoy playing chess, and I’m a huge movie buff – I can talk your ear off about the latest science fiction films. I also have a cat who keeps me company during late-night coding sessions. Through this talk, you’ll see how my interests in math and HPC intersect in the research I do. Now, let’s get into the outline of the presentation.

## Slide 4: Talk Outline

- **Introduction & Overview:** Polynomial regression and surrogate modeling – what are they and how do linear systems ($A\mathbf{x}=\mathbf{b}$) tie in? Key matrix decompositions (LU, QR, SVD) relevant to solving these models.
    
- **Historical Background:** Origins of polynomial regression (Legendre & Gauss and least squares)​[en.wikipedia.org](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=squares.%20The%20least,The%20first%20design%20of%20an), development of linear algebra techniques (Gaussian elimination, Householder’s QR​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization), SVD​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202)), and early surrogate modeling in science (e.g. response surface methods​[clas.iusb.edu](https://clas.iusb.edu/math-compsci/_prior-proposals/Nbradley_Proposal.pdf#:~:text=history,Moreover)).
    
- **Algorithms & Methods:** Formulating polynomial regression as a least-squares problem. Solving $A^T A \mathbf{x}=A^T\mathbf{b}$ via matrix factorization (normal equations vs. QR factorization vs. SVD). Illustrated with examples.
    
- **HPC Applications:** Use cases in High-Performance Computing – modeling performance of computational kernels (e.g. matrix multiplication (GEMM) and RAJAPerf benchmarks) with polynomial fits. How these surrogates aid in tuning HPC applications.
    
- **Implementation & Results:** Highlights from our ICCS 2025 paper on Surrogate-Based Modeling (SBM). Comparing polynomial surrogate models to k-Nearest Neighbors (kNN) for HPC performance data. Visualization of performance surfaces, error metrics (MSE, $R^2$), and the impact of polynomial degree on accuracy​file-7rhtrwtsjd5w8c8gletf19.
    
- **Open Issues & Future Work:** Current challenges in surrogate modeling for HPC – sampling strategies, generalizability across systems, dealing with non-linear “cliffs” in performance, avoiding overfitting, etc. Opportunities for improvement and research questions.
    
- **Discussion & Q&A:** Wrap up with references and open the floor for questions. Revisit test questions for discussion.

**Speaker Notes:**  
Here’s the roadmap for my talk. We’ll start with an **Introduction** to polynomial regression and surrogate modeling, establishing how linear algebra underpins these models. Then I’ll provide some **Historical Background**, touching on the key contributors and developments that got us where we are (from Legendre and Gauss’s least-squares method to modern HPC surrogate approaches). Next, we’ll dive into the **Algorithms & Methods** – how to set up the polynomial regression problem and solve it using linear algebra techniques like QR or SVD. After covering the theory, we move to **HPC Applications**, seeing how polynomial regression is applied to model the performance of real computational kernels (for example, how we predict the runtime of a matrix-multiplication operation on a supercomputer). Following that, I’ll share **Implementation & Results** from our recent research: we built surrogate models (polynomial fits) for HPC benchmark data and compared their performance to a traditional machine learning method (kNN). We’ll see some results like how increasing polynomial degree affected prediction accuracy and how our surrogate model outperformed kNN​file-7rhtrwtsjd5w8c8gletf19. Finally, we’ll discuss **Open Issues** – the challenges and open questions in this field – and conclude with a **Discussion/Q&A**, including revisiting those initial test questions to see if we’ve answered them.

## Slide 5: Overview – Polynomial Regression & Surrogates

- **Polynomial Regression:** A form of regression analysis where the relationship between input variables and an output is modeled as an $n$th-degree polynomial. Essentially an extension of linear regression that can fit curves. We seek coefficients $(a_0, a_1, ..., a_k)$ such that $y \approx a_0 + a_1 x + \cdots + a_k x^k$ for our data​[mathworld.wolfram.com](https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html#:~:text=Generalizing%20from%20a%20straight%20line,k%20th%20degree%20%2015).
    
- **Linear Algebra Formulation:** Fitting a polynomial of degree $k$ leads to an overdetermined linear system. We construct a **design matrix** $A$ (Vandermonde matrix with rows $[1, x_i, x_i^2, ..., x_i^k]$ for each data point $i$) and solve $A \mathbf{x} \approx \mathbf{b}$, where $\mathbf{x}=[a_0,\dots,a_k]^T$ are the polynomial coefficients and $\mathbf{b}=[y_1,...,y_n]^T$ are observed outputs​[mathworld.wolfram.com](https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html#:~:text=These%20lead%20to%20the%20equations). The optimal solution minimizes the sum of squared residuals $||A\mathbf{x}-\mathbf{b}||^2$.
    
- **Matrix Decompositions:** To solve $A\mathbf{x}=\mathbf{b}$ (or normal equations $A^T A \mathbf{x}=A^T \mathbf{b}$) reliably, we use linear algebra techniques:
    
    - _LU Decomposition:_ Factor $A$ or $A^T A$ into lower/upper triangular factors (less common for least-squares due to stability issues).
        
    - _QR Factorization:_ Decompose $A = Q R$ (with $Q$ orthogonal, $R$ upper triangular) to solve $R\mathbf{x}=Q^T\mathbf{b}$. Householder reflections (1958) provide a stable way to compute QR​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization).
        
    - _Singular Value Decomposition (SVD):_ Compute $A=U \Sigma V^T$ – gives the pseudo-inverse solution $\mathbf{x}=V \Sigma^{+} U^T \mathbf{b}$, very stable especially if $A$ is ill-conditioned. Efficient algorithms for SVD emerged in the 1960s (Golub & Kahan 1965​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202)).
        
- **Surrogate Modeling:** Using an approximate model to emulate a complex function or system. In HPC and simulation contexts, surrogate models (e.g. polynomial fits) provide fast predictions in place of expensive experiments. **Key idea:** sample the true system sparsely and fit a surrogate to predict outputs at untested points​file-7rhtrwtsjd5w8c8gletf19. Polynomial regression is a common surrogate model choice due to its simplicity and analytical tractability.
    

**Speaker Notes:**  
This slide gives an overview of the main concepts. **Polynomial regression** is simply fitting a polynomial curve through data points – it generalizes linear regression by allowing curved fits. For instance, instead of a straight-line fit $y = a_0 + a_1 x$, we might fit a quadratic $y = a_0 + a_1 x + a_2 x^2$, or higher degrees, to capture curvature in the data.

Crucially, we can express polynomial regression in a linear algebra framework. If we have data points $(x_i, y_i)$, $i=1\ldots n$, and we choose a polynomial degree $k$, we set up a **design matrix** $A$ where each row corresponds to a data point and contains $[1, x_i, x_i^2, ..., x_i^k]$. Our unknown coefficient vector $\mathbf{x}$ holds the polynomial coefficients, and the observed outputs go into vector $\mathbf{b}$. The fitting problem is $A \mathbf{x} \approx \mathbf{b}$. Typically $n$ (number of data points) is greater than $(k+1)$ (number of coefficients), so it’s an overdetermined system – we usually cannot solve it exactly, but we **minimize the least-squares error**. This leads to the normal equations $A^T A \mathbf{x} = A^T \mathbf{b}$.

Solving this efficiently and stably is where **matrix decompositions** come in. A naive solution uses $A^T A$ (normal equations) and then LU factorization, but $A^T A$ can be ill-conditioned. A better approach is to use **QR factorization** directly on $A$. Householder reflections (first described by Alston Householder in 1958) are a robust way to compute QR​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization), and this method avoids squaring the condition number like normal equations do. Another powerful approach is **SVD (Singular Value Decomposition)** – it not only solves the least squares problem in a stable way but also gives insight into the solution (it can handle rank-deficient cases gracefully by zeroing small singular values). Gene Golub and William Kahan developed a practical SVD algorithm in 1965​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202), which was a milestone in computational linear algebra.

Finally, **surrogate modeling** is the broader concept of which polynomial regression can be an example. In HPC, we often deal with functions or simulations that are extremely expensive to run for all possible inputs. Instead, we run a modest number of experiments and **fit a surrogate model** to those. The surrogate (here, a polynomial function) approximates the underlying behavior so we can predict outcomes without exhaustive measurement. This approach has been used to tune systems – for example, building a performance model of an application by sampling a few configurations and then predicting performance in other cases​file-7rhtrwtsjd5w8c8gletf19. Surrogate models strike a balance: they are much cheaper to evaluate than the real system, at the cost of some modeling error. The goal is to make that error small enough to still be useful.

With these basic ideas in mind, we’re ready to delve into how these techniques evolved and are applied.

## Slide 6: History and Key Developments

- **Legendre & Gauss – Least Squares (1805–1809):** The method of least squares, fundamental to regression, was first published by Adrien-Marie Legendre in 1805 and later by Carl Friedrich Gauss in 1809​[en.wikipedia.org](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=squares.%20The%20least,The%20first%20design%20of%20an). It provided a way to find the best-fit curve (or line) by minimizing squared errors. This set the stage for all polynomial regression techniques.
    
- **Early Linear Algebra (19th–20th Century):** Development of matrix theory enabled solving linear systems systematically. Gaussian elimination (Gauss/Jordan) became a standard method for solving $A\mathbf{x}=\mathbf{b}$. By mid-20th century, decompositions were introduced for numerical stability:
    
    - LU decomposition (algorithms by Doolittle, Crout, etc.) to factor matrices into lower and upper triangular form.
        
    - **Householder’s QR (1958):** Alston Householder demonstrated use of orthogonal reflections to safely compute QR factorization​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization), greatly improving least-squares solution stability.
        
    - **Singular Value Decomposition:** Although the concept of SVD existed in mathematics, an efficient algorithm came in 1965 (Golub & Kahan)​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202), allowing robust solution of linear systems and pseudoinverses.
        
- **Surrogate Modeling Origins:** In engineering and science, using polynomial approximations as “surrogate models” goes back to the 1950s. **Response Surface Methodology** (Box and Wilson, 1951) introduced fitting low-degree polynomials to experimental data to find optimal conditions​[clas.iusb.edu](https://clas.iusb.edu/math-compsci/_prior-proposals/Nbradley_Proposal.pdf#:~:text=history,Moreover). For example, they advocated using first-order (and later second-order) polynomial models to explore processes – an early form of surrogate modeling in the chemical industry.
    
- **Modern HPC Surrogates:** Surrogate models gained traction in computer performance in the 2000s. Notably, **Travis Johnston et al. (2015)** applied polynomial surrogate modeling to tune Hadoop/MapReduce jobs​file-7rhtrwtsjd5w8c8gletf19, demonstrating that a polynomial model could predict job execution time across configurations and yield near-optimal settings with far fewer experiments. In 2016, Johnston and colleagues introduced **HYPPO (Hybrid Piecewise Polynomial Modeling)** to handle non-smooth performance surfaces​file-7rhtrwtsjd5w8c8gletf19 – essentially using multiple polynomial segments to fit complex HPC performance data that a single global polynomial couldn’t capture.
    
- **Continuous Advancements:** The convergence of statistical regression and HPC performance tuning has led to tools that integrate these models into autotuning frameworks. Through the 2010s and 2020s, research by the HPC community (including our lab) has extended surrogate modeling to multi-dimensional parameter spaces and heterogeneous hardware. Our work builds on this foundation by applying polynomial regression surrogates to modern HPC benchmarks and refining techniques to improve accuracy and efficiency.
    

**Speaker Notes:**  
Let’s step back in time. The idea of fitting a model by **least squares** is over two centuries old. Adrien-Marie Legendre published the method in 1805, using it for astronomic and geodetic data, and Carl Gauss also formulated it (famously using it to predict a comet’s orbit) by 1809​[en.wikipedia.org](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=squares.%20The%20least,The%20first%20design%20of%20an). This was the birth of regression modeling – providing a mathematical way to find the best fitting curve by minimizing the sum of squared deviations. That’s the root of polynomial regression: it’s essentially applying least squares to a polynomial model.

On the linear algebra side, solving linear equations goes even further back (Gauss didn’t have matrices as we know them, but he solved normal equations by elimination). By the late 19th century, mathematicians like Cayley and Sylvester formalized matrix algebra. In the 20th century, we developed systematic methods: **Gaussian elimination** became the go-to for solving systems. In numerical analysis, improvements came to handle stability and efficiency. **LU decomposition** allowed solving systems quickly (once the matrix was factored, many $b$’s could be solved for cheaply). However, for least squares problems, $A^T A$ can be ill-conditioned, so direct LU on $A^T A$ could be unstable. That’s where **QR factorization** comes in – Householder’s 1958 paper introduced the use of orthogonal transformations (now called Householder reflections) to zero out parts of the matrix step by step​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization). This was a breakthrough for solving least squares reliably. Similarly, the **SVD** – while known theoretically – wasn’t practical until Golub & Kahan’s algorithm in 1965​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202), which made it feasible to compute singular values and vectors for moderately large matrices. These developments in linear algebra were crucial: as datasets got bigger and models more complex, we needed stable algorithms to compute the polynomial fits.

Parallel to these mathematical developments, the concept of **surrogate models** was emerging in experimental sciences. In 1951, statisticians George Box and K. Wilson introduced **Response Surface Methodology**, using polynomial equations as surrogates to guide chemical experiments​[clas.iusb.edu](https://clas.iusb.edu/math-compsci/_prior-proposals/Nbradley_Proposal.pdf#:~:text=history,Moreover). Instead of trying every combination of factors (which could be prohibitively expensive), they would fit a polynomial to a few experiments and use it to predict where optimum conditions might be – this is essentially surrogate modeling: a polynomial proxy for an underlying phenomenon.

Fast forward to recent decades, surrogate modeling has found a home in HPC. Systems and applications have many tunable parameters (thread counts, block sizes, input sizes, etc.), and testing all combinations on a supercomputer is infeasible. In 2015, researchers (Johnston et al.) used polynomial regression to tune MapReduce job configurations​file-7rhtrwtsjd5w8c8gletf19. They showed you can sample a handful of configurations, fit a polynomial model for job runtime, and then use that model to predict the best settings, **saving a ton of time** over brute force search. Building on that, in 2016 the same group tackled the issue that not all performance surfaces are smooth polynomials – sometimes adding more resources can cause sudden drops or plateaus in performance. They developed **HYPPO**, which essentially stitches together multiple polynomial models (piecewise) to better fit “bumpy” performance curves​file-7rhtrwtsjd5w8c8gletf19.

All these efforts lay the groundwork for our work. As hardware and applications evolve (think GPUs, multicore CPUs, etc.), surrogate models remain a powerful idea. The HPC community continues to refine these techniques – for example, integrating surrogate modeling with performance profiling tools, or developing adaptive sampling methods to build better models. In this talk’s context, we leverage this history to apply polynomial regression surrogates to a modern HPC benchmark suite and compare it with other approaches.

## Slide 7: Algorithms – Polynomial Regression via Least Squares

- **Setting up the Equations:** Given $n$ data points $(x_i, y_i)$, fitting a degree $k$ polynomial means finding $a_0,...,a_k$ minimizing $R^2 = \sum_{i=1}^n [y_i - (a_0 + a_1 x_i + \dots + a_k x_i^k)]^2$. Setting the derivative $\partial R^2/\partial a_j = 0$ for each coefficient yields the **normal equations**​[mathworld.wolfram.com](https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html#:~:text=The%20partial%20derivatives%20,are)​[mathworld.wolfram.com](https://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html#:~:text=These%20lead%20to%20the%20equations) – a linear system:
    
![[Pasted image 20250411170121.png]]

- This is equivalent to $A^T A \mathbf{x} = A^T \mathbf{b}$ where $A_{ij} = x_i^{,j-1}$. The normal equation matrix (Gram matrix) on left is symmetric and invertible if the $x$ values provide independent information (no multicollinearity).
    
- **Solving $A \mathbf{x} = \mathbf{b}$:** Directly solving the normal equations with Gaussian elimination is one approach, but can be prone to numerical error if $A^T A$ is ill-conditioned. Preferred methods:
    
    - **QR Factorization:** Decompose $A = Q R$. Because $Q$ is orthogonal, we have $Q^T A \mathbf{x} = Q^T \mathbf{b}$ $\implies$ $R \mathbf{x} = Q^T \mathbf{b}$. Since $R$ is upper-triangular with dimension $(k+1)\times(k+1)$, we can solve for $\mathbf{x}$ by back-substitution. QR avoids explicitly forming $A^T A$. Modern algorithms use Householder reflections or Givens rotations to compute $Q$ and $R$ stably​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization).
        
    - **Normal Equations with Cholesky:** If one does use $A^T A$, it can be factored as $A^T A = L L^T$ (Cholesky factorization) since it’s symmetric positive-definite (assuming full rank). Then solve $L (L^T \mathbf{x}) = A^T \mathbf{b}$. This is efficient but again, $A^T A$ might magnify condition problems.
        
    - **SVD Solution:** Compute $A = U \Sigma V^T$. The least-squares solution is $\mathbf{x} = V \Sigma^+ U^T \mathbf{b}$, where $\Sigma^+$ inverts the nonzero singular values in $\Sigma$. This handles rank-deficient cases gracefully by dropping tiny singular values (effectively regularizing the solution). It’s the most robust method, though computationally a bit heavier. Golub and Kahan’s algorithm (1965) made this feasible for moderate $n$​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202).
        
- **Example Illustration:** (Figure) Suppose we have points and we fit a 2nd-degree polynomial (parabola). We set up $A$ with columns $[1, x, x^2]$. Solving yields coefficients $(a_0,a_1,a_2)$. The figure shows data points (blue dots) and the fitted parabola (red curve) passing near them. The least-squares solution balances the errors above and below the curve. If we added a new data point, the system $A\mathbf{x}=\mathbf{b}$ would expand and we’d recompute a new best fit. This process generalizes to higher-degree polynomials and multiple variables (we’d include terms like $x, x^2, xy,$ etc., still a linear system in the coefficients).
    
- **Computational Cost:** Solving a polynomial regression is $O(n k^2)$ with QR or $O(n k + k^3)$ with normal equations (Cholesky), where $n$ is number of samples and $k$ the polynomial degree. For small $k$, this is very fast even for large $n$. In HPC contexts, $n$ might be large (many sample points), but we can leverage optimized linear algebra libraries (BLAS/LAPACK, or GPU-accelerated libraries like MAGMA) to perform these factorizations efficiently​file-7rhtrwtsjd5w8c8gletf19.
    

**Speaker Notes:**  
This slide goes step-by-step through the process of deriving and solving the polynomial regression equations. We start with the least-squares setup: we have $R^2$, the sum of squared residuals, as a function of the coefficients $a_j$. Taking derivatives and setting them to zero gives us a system of linear equations – these are the **normal equations**. I’ve written a generic form of them here. It might look a bit intimidating, but notice the pattern in that big matrix: it’s full of sums of powers of $x_i$. That matrix is essentially $A^T A$ for the Vandermonde matrix $A$. Solving those equations yields the optimal coefficients $a_0 \dots a_k$. Historically, people would literally compute those sums and solve the equations by hand or simple elimination for small data sets!

In practice, we rarely form that whole normal-equation matrix explicitly today. We have more robust ways. The **QR factorization** approach is typically the go-to for solving least squares – it’s numerically stable. The idea is we augment matrix $A$ with the $b$ vector and perform orthogonal transformations (Householder reflections) that zero out the sub-diagonal entries. By the end, we get an upper-triangular system $R x = Q^T b$. It’s efficient and avoids squaring the condition number of $A$. **Cholesky factorization** of $A^T A$ is another way: if we do go the normal equation route, Cholesky is faster than generic Gaussian elimination since it exploits symmetry, taking about half the operations. But again, the caveat is numerical stability – if $A$ has a high condition number, $A^T A$ has its condition number squared, which can lead to big rounding errors.

**SVD** is the gold standard if we need the most accurate solution. It will reliably handle cases where some columns of $A$ are almost linearly dependent (for example, fitting a very high-degree polynomial can cause columns $[x^j]$ to be nearly collinear for broad ranges of $x$). The SVD will indicate if any singular values are nearly zero – a sign that the polynomial degree might be too high for the data – and you could truncate those to avoid overfitting. In fact, using SVD is a way to implement **ridge regression** or regularization by damping small singular values. The downside is SVD is a bit slower ($O(n^2 k)$ typically), but for our problem sizes (in modeling HPC performance, $k$ is usually not huge), it’s manageable.

I also want to emphasize the computational aspect: if we have, say, $n=1000$ sample points and we’re fitting a 5th-degree polynomial ($k=5$), solving via QR is extremely fast on any modern machine. Even larger, if $n$ is tens of thousands, that’s fine. In HPC scenarios, sometimes $n$ could be in the thousands (depending on how many experiments we can run). We can use optimized libraries – e.g. **MAGMA** for GPU-accelerated linear algebra – to solve these systems even faster​file-7rhtrwtsjd5w8c8gletf19. In our work, we indeed leveraged such libraries to handle the computations.

The little example mentioned on the slide is to give intuition: imagine plotting some measured data and fitting a curve. The math ensures that the curve is the “best compromise” through the points in a least-squares sense. We’ll soon see this in action with real HPC data.

## Slide 8: Applications in HPC – Performance Modeling

- **Why Model Performance?** HPC applications have many tunable parameters (thread counts, problem sizes, memory layouts, etc.). Exhaustively testing all combinations on a supercomputer is infeasible. Polynomial regression offers a quick predictive model: given a few measurement points, we can **predict performance at new parameter settings**, guiding tuning decisions without full trials. Surrogate models thus save time and compute resources.
    
- **Case Study – GEMM (Matrix Multiply):** General Matrix Multiply is a core HPC kernel (in LINPACK, deep learning, etc.). Its performance (e.g., GFLOP/s or runtime) as a function of matrix size or thread count can be modeled. For instance, one can run GEMM on various matrix sizes and fit a polynomial to the runtime vs. size data. A well-fitted polynomial might capture how runtime grows roughly cubicly with dimension before hitting memory bandwidth limits (causing sub-cubic growth). This helps predict execution time for sizes not directly tested. (Chart: plot of GEMM runtime vs. matrix size with a polynomial fit illustrating the trend.)
    
- **RAJA Performance Suite (RAJAPerf):** RAJAPerf is a set of HPC kernel benchmarks designed to test performance portability (developed by LLNL/UTK)​file-7rhtrwtsjd5w8c8gletf19. It includes kernels categorized as:
    
    - _Compute-bound_ (e.g., dense math operations, high FLOP/byte, like a dot product or small matrix ops),
        
    - _Memory-bound_ (e.g., STREAM Triad, where performance is limited by memory bandwidth),
        
    - _Hybrid-bound_ (mix of compute and memory).  
        We apply polynomial models to such kernels. For example, **STREAM Triad** (memory-bound vector update) performance vs. number of threads on a CPU: we found it follows a curve that rises then plateaus (limited by memory bandwidth). A polynomial of moderate degree can fit this behavior, peaking at the saturation point.
        
- **Multi-Parameter Surrogates:** In some cases we have more than one input variable. E.g., an HPC kernel’s performance might depend on **two parameters** – say, problem size _and_ number of threads. Then the surrogate might be a polynomial in two variables (a surface fit). For instance, a simple second-order model: $t = \beta_0 + \beta_1 \text{(threads)} + \beta_2 \text{(size)} + \beta_{11} \text{(threads)}^2 + \beta_{22} \text{(size)}^2 + \beta_{12}(\text{threads}\cdot \text{size})$. We solve a linear system for $\beta$’s in similar fashion. These polynomial surface models can approximate, say, how performance changes when we simultaneously vary the number of CPU cores and the data size of a kernel.
    
- **Visualization:** (Figures) For a given kernel, we can visualize actual measured performance vs. predicted. One common approach: show a 3D surface where X and Y axes are two tunable parameters and Z is performance (e.g., execution time). We overlay the surrogate polynomial surface on scatter points of real measurements. Ideally, the polynomial surface closely follows the trend of the real data points – indicating a good fit. In our HPC use cases, we often see smooth surfaces that polynomials can capture (though sometimes with a high-degree). The RAJAPerf benchmarks we used have such surfaces: for example, **Basic TRAP Int** kernel (an integer computation loop) on a CPU yields a curved performance surface across thread count and problem size, which our polynomial model was able to approximate well.
    

**Speaker Notes:**  
Now let’s talk about _applications in HPC_. The big question is: why do we care about building these polynomial models in the context of supercomputers? The answer is **performance tuning and prediction**. In HPC environments, users and system designers are constantly trying to optimize how code runs – to squeeze out more speed. There are so many parameters: how many threads or MPI ranks to use, how to distribute data, which algorithmic variant to choose, etc. Trying all possible combinations on a large system could take an absurd amount of computing time (and energy!). Instead, we use **surrogate models** to approximate the performance landscape. By measuring performance at a limited set of points (for example, run the program on a few representative configurations), we can fit our polynomial and use it to _interpolate or even extrapolate_ performance at other settings. This approach has proven useful to guide practitioners toward optimal settings without brute force search.

As a concrete example, take the case of **GEMM (General Matrix Multiply)**, which is basically matrix multiplication – a building block in many HPC libraries. Suppose we want to model how long GEMM takes based on the matrix size $N$ on a certain machine. We know theoretically, for large $N$, the operation is $O(N^3)$ flops, but on a real machine cache and memory bandwidth come into play. We can run GEMM for a range of sizes (say $N=1000, 2000, ..., 10000$) and record the runtime. If we plot that, it might look somewhat cubic initially but then slower growth as memory limits kick in. A polynomial regression might find, for instance, a best-fit of degree 3 or 4 that captures that curve. The polynomial might not be _exactly_ $N^3$, but perhaps something like $a_3 N^3 + a_2 N^2 + a_1 N + a_0$ that fits the measured data. With that polynomial, if a user asks “how long for $N=12000$?”, we can answer without actually running it. This is a simple one-parameter surrogate.

In our research, we dealt with the **RAJAPerf suite** – a collection of small kernel benchmarks representative of real HPC codes​file-7rhtrwtsjd5w8c8gletf19. Each kernel can be run with different parameters (like varying number of threads, or running on CPU vs GPU). We gathered performance data for some of these kernels on different hardware. For example, **STREAM Triad** (which adds two vectors and multiplies by a scalar) is _memory-bound_ – meaning adding more CPU cores helps up to a point, then you hit the memory bandwidth limit of the system and adding more threads doesn’t increase speed. If we plot throughput (like GB/s) vs number of threads, it’s an increasing curve that flattens out. A polynomial of appropriate degree can model that “speedup then plateau” shape. Similarly, **Basic TRAP Int** (an integer-heavy loop) might scale more linearly with threads until other factors limit it. By fitting polynomials, we obtained surrogate models for each kernel’s performance behavior. Some needed higher-degree polynomials to capture nuances (especially on CPU, where performance curves had more bends), while others (on GPU, often simpler – saturating quickly) needed only low-degree.

For multi-parameter situations – for instance, simultaneously varying _problem size_ and _threads_, the model becomes a surface. We still solve it via linear algebra (the design matrix now has columns for $x$, $y$, $xy$, $x^2$, etc.). Visualizing these is insightful: in our work we produced 3D surface plots of predicted performance. One of the figures (which I’ll show later) presents the predicted performance surface from our polynomial model for each kernel. The nice thing is that these surfaces were very close to the actual measured data points​file-7rhtrwtsjd5w8c8gletf19 – showing that the polynomial surrogate captured the performance landscape well. This means once we have that polynomial, we can answer questions like “what if I use 32 threads on a problem of size 10,000?” instantly, rather than needing to run that experiment. In a scenario where running on a supercomputer queue might take hours, having a surrogate model can be incredibly useful to explore “what-if” scenarios quickly.

## Slide 9: Implementation & Results – Surrogate-Based Modeling in HPC

- **Our Approach (SBM):** We built surrogate models for HPC kernel performance using polynomial regression – calling this approach **Surrogate-Based Modeling (SBM)**. We used RAJAPerf benchmark data (on both CPU and GPU) to train polynomials and evaluated how well they predict performance. We also compared against a non-parametric method, k-Nearest Neighbors (**kNN**), as a baseline.
    
- **Polynomial Degree Tuning:** A critical choice is the degree of the polynomial. Too low, and the model can’t capture the performance curve; too high, and it overfits noise. We systematically tried degrees 1 through 13 on each dataset. **Result:** On CPU kernels, prediction accuracy kept improving up to around degree 10 (with $R^2$ approaching 0.96–0.98) but then worsened slightly beyond that due to overfitting​file-7rhtrwtsjd5w8c8gletf19. On the GPU kernel, a much lower degree (around 4) was sufficient to reach $R^2 \approx 1.0$ (the performance trend was simpler)​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. The plot of Mean Squared Error (MSE) vs. degree showed a clear “elbow” at the optimal degree (degree > 10 on CPU started to increase MSE again)​file-7rhtrwtsjd5w8c8gletf19. **Takeaway:** we selected degree 10 for CPU models and 4 for the GPU model as the sweet spot between accuracy and generalization.
    
- **Accuracy vs. kNN:** Using the optimal degrees, we compared SBM (polynomial surrogate) and kNN on a test set of performance points. **Outcome:** The polynomial surrogate consistently yielded better accuracy on the CPU kernels – up to **54% lower error** than kNN​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. For example, on a CPU kernel, our model had an average MSE of ~0.046 (with $R^2 \approx 0.96$) whereas kNN’s MSE was ~0.198 ($R^2 \approx 0.94$)​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. Also, the variance (stability) of errors across multiple runs was lower with the polynomial model​file-7rhtrwtsjd5w8c8gletf19. On the GPU kernel (which had a very smooth surface), both methods performed similarly (essentially near-perfect fits, since the data pattern was straightforward)​file-7rhtrwtsjd5w8c8gletf19.
    
- **Sample Efficiency:** Another advantage observed – the polynomial model reached high accuracy with fewer training samples. We found we could cut the training data by about **33%** and still maintain accuracy with SBM, whereas kNN needed more data to achieve the same accuracy​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. In practical terms, to get an accurate model of a certain kernel, we might need 2200 sample points for SBM vs. 2700 for kNN, saving dozens of experiments. This is important in HPC where each data point might be an expensive run on a cluster.
    
- **Visualization of Results:**
    
    - _Polynomial Fit Quality:_ We plotted the predicted vs. actual performance for each kernel. The polynomial surrogate’s predictions closely lined up with the actual measurements (most points falling near the diagonal line). For CPU kernels, there were a few slight deviations but overall $R^2 > 0.95$, indicating the model explains >95% of the variance​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. For the GPU kernel, points were essentially on the line ($R^2 \approx 1.0$).
        
    - _Performance Surfaces:_ **Figure –** The 3D surfaces generated by the polynomial models are shown for four representative cases: (a) Basic TRAP INT (CPU), (b) Basic INIT VIEW1D (CPU), (c) Stream TRIAD (CPU), (d) Stream TRIAD (GPU). These surfaces (in orange mesh) align very well with the actual measured data points plotted (blue dots), demonstrating the surrogate’s accuracy​file-7rhtrwtsjd5w8c8gletf19. Notably, surfaces (a), (b), (c) have more curvature and complexity, captured by the higher-degree polynomial, whereas surface (d) is relatively simple (flat region after an initial rise), captured by a low-degree polynomial.
        
    - _Error Metrics vs. Degree:_ We include a 2D plot of MSE and $R^2$ as a function of polynomial degree for a sample kernel. It shows MSE dropping and $R^2$ rising as the degree increases, then MSE bottoming out around degree 10 and slightly rising if degree goes to 12 or 13 (sign of overfit)​file-7rhtrwtsjd5w8c8gletf19. This visual reinforced our degree selection strategy.
        

**Speaker Notes:**  
Now I’ll share the key results from our **Surrogate-Based Modeling (SBM)** study (this is from our ICCS 2025 paper). The goal was to see how well polynomial regression can model HPC application performance and how it stacks up against a simpler machine learning approach (kNN). We used data from the RAJAPerf suite – specifically three kernels on a CPU and one on a GPU, covering different performance patterns.

One of the first things we tackled was: _what polynomial degree to use?_ We don’t want to just guess this – we systematically tested degrees from 1 up to 13. For each degree, we trained the polynomial on all data and checked the fit error. The trend was very interesting: on CPU kernels, error kept improving as we increased degree, until about degree 10, after which additional complexity started to hurt​file-7rhtrwtsjd5w8c8gletf19. This is the classic overfitting behavior: by degree 12 or 13 the polynomial can start fitting noise or minor fluctuations that don’t generalize. On the GPU kernel, the behavior was different – after degree ~4, the model was basically perfect (because the GPU performance curve was pretty smooth and simple), so higher degrees didn’t improve it (and in fact, we chose degree 4 to avoid any risk of overfit)​file-7rhtrwtsjd5w8c8gletf19. So our chosen degrees moving forward were 10 for the CPU cases and 4 for the GPU case. This was a nice confirmation that **more complex is not always better** – you want just enough polynomial flexibility to capture the trend but not so much that you chase noise.

Next, the head-to-head comparison: **Polynomial Surrogate vs. kNN.** We partitioned data into training and testing (and also did cross-validation to be thorough). The bottom line: the polynomial model outperformed kNN on all the CPU kernels by a significant margin​file-7rhtrwtsjd5w8c8gletf19. For example, for one kernel, our model’s $R^2$ was about 0.96, whereas kNN achieved about 0.89–0.94​file-7rhtrwtsjd5w8c8gletf19. In terms of MSE (mean squared error), lower is better: we had around $0.046$ (which is like 4.6% of the variability left unexplained, roughly) and kNN had $0.19$ (19%) in one case​file-7rhtrwtsjd5w8c8gletf19. That’s a big difference – in fact up to **54% improvement in accuracy** as we noted​file-7rhtrwtsjd5w8c8gletf19. On the GPU kernel, both methods were nearly perfect ($R^2 \approx 1$ for both)​file-7rhtrwtsjd5w8c8gletf19 – which makes sense because that data was so clean and easy that even a simple interpolation (kNN) could handle it.

Another aspect is how **data-efficient** the methods are. We found that to reach a high accuracy, our polynomial model didn’t need as many sample points. We could drop about one-third of the training points and still predict well​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. kNN, on the other hand, inherently relies on having dense coverage of the space (because it predicts by local interpolation between nearest neighbors). So if data is sparse, kNN can struggle or require more points to fill gaps. In a scenario where obtaining each data point means running a program on a supercomputer for an hour, needing fewer points is a big win. Our surrogate model effectively “extrapolated” some areas in between points accurately, whereas kNN would have needed an actual neighbor point there to interpolate.

To make this tangible, we have some visuals (imagine these as I describe them): We plotted the **predicted vs actual** performance for each method. The polynomial model’s points hug the diagonal — that’s what gave those high $R^2$ values. The kNN had more scatter around the diagonal for the harder cases. We also plotted the **performance surfaces**: these are like 3D hills or landscapes showing performance over a range of two parameters. In the figure included, the orange mesh surfaces are what our polynomial predicts, and the blue dots are actual measurements. You can see they match closely​file-7rhtrwtsjd5w8c8gletf19. For the GPU case, the surface is quite simple (basically a flat plane after a certain point), and indeed a low-degree polynomial got that exactly. For the CPU, the surfaces have a bit more structure (like maybe a gentle hill shape) and our higher-degree polynomial was able to trace that shape.

Lastly, the **error vs. degree** plot we included is a great summary of the overfitting point: you see error going down steeply up to a certain polynomial degree, then leveling off, and even rising if you go further. It validates the common wisdom: use cross-validation or similar to pick a polynomial degree that’s not too low (underfitting) or too high (overfitting). We did that and it paid off in stable, accurate predictions.

In summary, these results show that **polynomial regression is a robust surrogate model** for these HPC performance data sets – it beat out a basic machine learning approach and did so with fewer data. It’s also interpretable (we can examine coefficients, etc.), which is a bonus. Next, I’ll discuss some of the broader lessons and open questions that come out of this.

## Slide 10: Open Issues and Future Work

- **Optimal Sampling Strategies:** How to choose sample points for building the surrogate? In our study we largely used evenly spaced or random samples. Smarter sampling (e.g. Latin Hypercube, adaptive sampling that adds points where error is high) could achieve the same accuracy with even fewer runs. This is an open research area: balancing exploration vs. exploitation in the parameter space to best train models.
    
- **Model Generalizability:** HPC systems evolve – a model trained on one architecture might not directly apply to another (e.g., CPU vs GPU differences). How can we make surrogate models that generalize or can be transferred? Possibly by including hardware characteristics as input features, or developing multi-fidelity models. In our case, we had to build separate models per hardware type. Future work could explore _generalized surrogates_ that incorporate platform parameters.
    
- **Handling Non-Smooth Behavior:** Not all performance surfaces are smooth polynomials. We observed certain phenomena (e.g., sudden drops in performance when hyper-threading kicks in, or NUMA effects) that a single polynomial may struggle with. Techniques like piecewise polynomial modeling (HYPPO) or adding indicator variables for regime changes could help​file-7rhtrwtsjd5w8c8gletf19. Determining when to split the model (and where) remains a challenge – it could be guided by error analysis (if residuals show systematic pattern, maybe a single model is insufficient).
    
- **Preventing Overfitting:** As we increase polynomial complexity, overfitting is a risk, especially if the training data has noise or measurement variability. We mitigated this by cross-validation and picking an optimal degree, but in more complex scenarios one might use regularization (ridge regression, LASSO) to penalize overly wiggly polynomials. Future surrogate modeling frameworks might incorporate an automatic complexity control (e.g., stop increasing degree when validation error starts rising). Ensuring the model doesn’t just memorize the training points but captures the true underlying trend is crucial for trust in predictions.
    
- **Integration with HPC Workflows:** For surrogate models to be widely useful, they should integrate with tools that HPC users already use. One idea is hooking these models into job schedulers or autotuning software – so that when a user submits a job, the system can suggest optimal settings or predict runtime using existing surrogate models. Another open question: can we update surrogate models on the fly? (For example, as new performance data comes in from production runs, refine the polynomial model without retraining from scratch.) This veers into _online learning_ territory and is a potential future direction.
    
- **Beyond Polynomials:** While our focus was polynomial regression, there are other surrogate methods (Gaussian Process regression, neural networks, splines, etc.). Polynomials have the advantage of simplicity and low computational cost, but more complex surrogates might capture certain patterns better (at the cost of more compute and risk of overfitting with too many parameters). An open research question is identifying for a given problem which surrogate type is “best” – perhaps an ensemble could even be used. In HPC especially, where data might be noisy or irregular, combining polynomial models with, say, a Gaussian Process (which gives uncertainty estimates) might be beneficial​file-7rhtrwtsjd5w8c8gletf19.
    
- **Reproducibility and Robustness:** HPC performance data can have run-to-run variance (due to network jitter, OS interference, etc.). This noise can confuse models. Techniques to make surrogates robust to noise – e.g., training on summary statistics or incorporating error bars – are worth exploring. We want surrogates that are not brittle: if the next run is slightly slower due to a hiccup, the model should not be thrown off. Over many runs, the model should represent the _expected_ performance.
    

**Speaker Notes:**  
Although our results are encouraging, there are several **open issues and opportunities for future work** in surrogate-based performance modeling for HPC:

- **Sampling**: We somewhat glossed over how we pick the sample points for training our model. In our experiments, we took a reasonably dense grid of points (or random selection) for each parameter. But this might not be optimal. There’s ongoing research into _adaptive sampling_ – essentially algorithms that decide, based on the current model, where to measure next to improve the model the most. Imagine starting with just a few points, fitting a polynomial, then seeing where the uncertainty or error is highest and sampling there next. This could dramatically reduce the number of runs needed. In HPC, where each data point can be costly, this is a big deal. We didn’t implement adaptive sampling in this study, so it’s a ripe area to extend our work.
    
- **Generalizability**: Right now, our surrogate for, say, “Stream Triad on CPU X” is specific. If you go to a different CPU or a GPU, you need a new model. It would be nice if we could generalize across platforms. Maybe by including machine parameters (like memory bandwidth or clock speed) as part of the input, we could have a meta-model that works for a _family_ of machines. This is somewhat uncharted territory. Some approaches use transfer learning or multi-task learning to share data between models. We’re interested in exploring whether one could train a surrogate on, say, one CPU and with minimal extra data adapt it to another CPU.
    
- **Non-smooth behavior**: HPC performance sometimes has these discontinuities – for example, when you saturate an interconnect or when an algorithm’s behavior changes at a threshold. A single polynomial is continuous and differentiable, so it can’t represent a sudden jump or corner well. Our approach in this paper was to keep the model relatively simple and avoid those regions, or in one case we might just accept a bit of error. But there’s an opportunity to combine models: one could detect if the residuals indicate a jump and then use a piecewise model (like two polynomials, one for before the jump, one after). The tricky part is algorithmically determining those segments – one might borrow techniques from change-point detection. The HYPPO paper I mentioned earlier dealt with this by manually partitioning based on known thresholds (they knew where performance changed regime)​file-7rhtrwtsjd5w8c8gletf19. In a general setting, we’d like the computer to figure it out.
    
- **Overfitting control**: We handled this by choosing polynomial degree carefully. But another way is adding a regularization term. In machine learning, it’s common to penalize large coefficients (which typically correspond to overly wiggly polynomials) – that’s ridge regression or LASSO. We could incorporate that to automatically keep the model smoother. Additionally, as we collect more data, we should retrain on fresh random splits to ensure our model isn’t just memorizing quirks of one particular data partition. Overfitting is especially sneaky in HPC because sometimes one outlier run (maybe a network hiccup causing a slow run) can mislead a high-degree polynomial to wiggle up to meet that outlier. We need to be cautious and maybe treat outliers in the data (we could do robust regression that downweights them).
    
- **Integration and automation**: In an ideal world, an HPC user wouldn’t manually do polynomial regression; the system or an autotuner would do it behind the scenes. There are tools out there that try different configurations and measure performance (autotuning frameworks). We see potential in incorporating our surrogate modeling approach into such tools. For example, an autotuner could run a handful of benchmarks, fit a polynomial model in real-time, and then use that model to suggest the next configuration to try or even directly jump to the predicted optimum. We’d love to see this become a part of HPC job schedulers or resource managers – imagine if when you submit a job, the system uses a model to predict how long it will run or what the best settings are, and informs you or automatically sets those. That’s a bit futuristic but not too far-fetched.
    
- **Exploring other models**: While we have been singing the praises of polynomials, we are aware that they’re not a panacea. For very complex performance patterns, something like a Gaussian Process (GP) might model subtle curves better and also give uncertainty estimates (so it can say “I’m not sure in this region, need more data”). GPs scale poorly with data size though, which is why polynomials are nice. Neural networks could fit almost anything given enough data, but they require a lot of data and tuning themselves – and interpretability drops. So there’s a trade-off. One research question is: for a given HPC problem, how to choose the modeling technique? We chose polynomials for simplicity and because earlier work indicated they work well for many cases​file-7rhtrwtsjd5w8c8gletf19. But part of future work could be dynamically selecting model type (maybe start with polynomial, if residuals look random – good; if residuals show a structured but non-poly pattern – maybe switch to a different function basis or a spline). There’s interesting work in _ensemble modeling_ – combining predictions from multiple models – which could also be applied to get more robust predictions.
    
- **Robustness to noise**: HPC runs can have variability. If our training data is noisy (perhaps +/-5% variation from run to run), how do we ensure the surrogate is learning the signal and not the noise? Techniques like averaging multiple runs per point, or explicitly modeling the noise (like assuming an error term) can help. In our study, we actually observed very low noise on our test system (running each configuration multiple times gave nearly identical results, which is nice). But on other systems or applications, that might not be true. Incorporating uncertainty in our models (maybe using a Bayesian approach or at least capturing variance) is another aspect to consider moving forward.
    

In summary, while we demonstrated a successful application of polynomial surrogates in HPC, there’s plenty of room to make these models more _adaptive, general, and integrated_. These are some of the directions we’re excited about exploring next.

## Slide 11: References

1. **Legendre (1805) – Least Squares:** Legendre, A.-M. _“Nouvelles méthodes pour la détermination des orbites des comètes.”_ (1805). – First publication of the least-squares method​[encyclopediaofmath.org](https://encyclopediaofmath.org/wiki/Legendre,_Adrien-Marie#:~:text=Summary,grounds%20by%20Gauss%20and%20Laplace), later justified by Gauss (1809)​[en.wikipedia.org](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=squares.%20The%20least,The%20first%20design%20of%20an) as the optimal solution for linear regression under the Gauss–Markov theorem.
    
2. **Householder (1958) – QR Factorization:** Householder, A. S. _“Unitary Triangularization of a Nonsymmetric Matrix.”_ Journal of the ACM, 5(4):339–342, 1958. – Introduced Householder transformations for QR decomposition, enabling stable solution of linear least-squares problems​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization).
    
3. **Golub & Kahan (1965) – SVD Algorithm:** Golub, G. H., and Kahan, W. _“Calculating the Singular Values and Pseudo-Inverse of a Matrix.”_ Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2):205–224, 1965. – Described the algorithm to compute SVD, laying groundwork for reliable least-squares via pseudoinverse​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202).
    
4. **Box & Wilson (1951) – Response Surface Methodology:** Box, G. E. P., and Wilson, K. B. _“On the Experimental Attainment of Optimum Conditions.”_ Journal of the Royal Statistical Society, Series B 13(1):1–45, 1951. – Pioneered surrogate modeling in engineering by using first- and second-order polynomials to approximate experimental response surfaces​[clas.iusb.edu](https://clas.iusb.edu/math-compsci/_prior-proposals/Nbradley_Proposal.pdf#:~:text=history,Moreover).
    
5. **Johnston et al. (2015) – Surrogate for MapReduce:** Johnston, T., Alsulmi, M., Cicotti, P., Taufer, M. _“Performance Tuning of MapReduce Jobs using Surrogate-Based Modeling.”_ In Proc. of ICCS 2015, vol. 51, pp. 49–59. – Applied polynomial surrogate models to big data (Hadoop) job tuning, demonstrating improved scalability and reduced sampling needs​file-7rhtrwtsjd5w8c8gletf19.
    
6. **Johnston et al. (2016) – HYPPO (Piecewise Surrogates):** Johnston, T., Zanin, C., Taufer, M. _“HYPPO: A Hybrid, Piecewise Polynomial Modeling Technique for Non-Smooth Surfaces.”_ In Proc. of SBAC-PAD 2016, pp. 26–33. – Addressed modeling of irregular HPC performance landscapes by using multiple polynomial segments for different regions​file-7rhtrwtsjd5w8c8gletf19.
    
7. **Bogale et al. (2025) – SBM for HPC Performance:** Bogale, B., Lumsden, I., Sukkari, D., Yokelson, D., Brink, S., Pearce, O., Taufer, M. _“Surrogate Models for Analyzing Performance Behavior of HPC Applications Using RAJAPerf.”_ In Proc. of ICCS 2025 (to appear). – Our study introducing Surrogate-Based Modeling (SBM) for HPC kernels. Demonstrated that a polynomial regression surrogate can predict performance with up to 54% higher accuracy using 33% fewer samples compared to kNN​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19. Showed optimal polynomial degrees (≈10 for CPU, 4 for GPU) balance accuracy and overfitting​file-7rhtrwtsjd5w8c8gletf19.
    
8. **RAJAPerf Suite (2024) – Benchmark Description:** Pearce, O., et al. _“RAJA Performance Suite: Performance Portability Analysis with Caliper and Thicket.”_ In Proc. of PMBS Workshop at SC 2024. – Describes the RAJA Perf Suite of kernels and their classification into compute-, memory-, and hybrid-bound categories​file-7rhtrwtsjd5w8c8gletf19, which provided the context for our surrogate modeling targets.
    
9. **Scikit-Learn (2021) – Regression in Python:** Pedregosa, F., et al. _“Scikit-learn: Machine Learning in Python.”_ JMLR 12:2825–2830, 2011. – (Tool reference) We utilized Python’s scikit-learn library for baseline models like kNN and to assist in polynomial regression fitting, leveraging its linear regression (least squares) implementation.
    
10. **Yang et al. (2005) – Cross-Platform Prediction:** Yang, L., Ma, X., Mueller, F. _“Cross-Platform Performance Prediction of Parallel Applications Using Partial Execution.”_ In Proc. of SC 2005. – An example of an alternative approach to performance modeling (partial execution + extrapolation)​file-7rhtrwtsjd5w8c8gletf19. Highlights the diversity of methods for performance prediction in HPC, against which surrogate modeling approaches like ours can be contrasted.
    

**Speaker Notes:**  
This slide lists references for the key sources and prior works mentioned throughout the talk. I want to emphasize a few of them:

- References 1–3 cover the foundational mathematical contributions (Legendre/Gauss for least squares​[en.wikipedia.org](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=squares.%20The%20least,The%20first%20design%20of%20an), Householder for QR​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization), Golub & Kahan for SVD​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202)). These laid the theoretical groundwork that makes everything we did possible – without stable linear algebra algorithms, fitting large models wouldn’t be feasible.
    
- Reference 4 (Box & Wilson, 1951) is a nod to the origins of surrogate modeling in experimental design​[clas.iusb.edu](https://clas.iusb.edu/math-compsci/_prior-proposals/Nbradley_Proposal.pdf#:~:text=history,Moreover). It’s interesting to see how ideas flow from statistics into computing.
    
- References 5 and 6 are from our lab’s prior work. Johnston et al. 2015​file-7rhtrwtsjd5w8c8gletf19 is essentially a direct precursor to what we’re doing, but in the big data domain (MapReduce jobs). And Johnston et al. 2016 (HYPPO)​file-7rhtrwtsjd5w8c8gletf19 tackles a problem we discussed – piecewise modeling for non-smooth cases.
    
- Reference 7 is our ICCS 2025 paper – that’s the core of this talk, showing the results we achieved with polynomial surrogates in HPC.​file-7rhtrwtsjd5w8c8gletf19​file-7rhtrwtsjd5w8c8gletf19 Many of the numbers and findings I presented (54% accuracy gain, 33% fewer samples, degree 10 vs 4, etc.) come straight from there.
    
- I also included Reference 8 about RAJAPerf​file-7rhtrwtsjd5w8c8gletf19, which is a technical report/paper describing the benchmarks themselves. That’s more for context – if anyone is interested in what these kernels are or how they’re characterized, that’s a good source.
    
- Reference 9 is a tool reference; we used scikit-learn’s implementations for some modeling (like kNN and linear regression for validation). It’s standard, but I list it for completeness.
    
- Reference 10 (Yang et al. 2005) is an example of a different approach to performance modeling​file-7rhtrwtsjd5w8c8gletf19 – using partial execution of code to predict full execution performance. I include it to acknowledge that surrogate modeling isn’t the only game in town; HPC performance prediction has other techniques. It’s always good to see our work in context with alternative methods.
    

For anyone interested in diving deeper, these references provide a trail from the classical underpinnings to the latest research in HPC performance modeling. I’d be happy to discuss any of them in more detail if there are questions.

## Slide 12: Discussion and Q&A

- **Discussion Prompt:** Given an HPC application you’re familiar with, how might you apply surrogate modeling to it? What would be the challenges (choosing inputs, collecting data, etc.) and potential benefits?
    
- Consider the test questions from the beginning – do you feel you can answer them now? (Let’s discuss them!)
    
- **Your Questions:** Please feel free to ask about any aspect – the theory, the implementation, limitations, or how this might extend to other scenarios.
    
- _(Alternatively, this slide can remain blank as an invitation for live questions.)_
    

**Speaker Notes:**  
We’ve covered a lot of ground, so now I’d like to open up for discussion. One question to consider: think about an HPC application or any performance-critical system you know. Could a polynomial surrogate model help in predicting or tuning its performance? What obstacles do you foresee in building such a model (for instance, do you have easy access to training data? is the performance behavior very irregular or noisy?) and what benefits might you get (maybe saving computation, or gaining insight into performance drivers)?

Also, let’s recall the initial **test questions** I posed. This is a good time to answer them together and ensure everything was clear:

- _Formulation:_ We saw that polynomial regression can be formulated as $A x = b$ by building the design matrix of input powers. And we solve it typically with QR or SVD for stability (that answers Q1).
    
- _HPC Motivation:_ The reason we do this in HPC is to avoid exhaustive tuning; a surrogate model is much cheaper once trained and can guide us to optimal configurations (answering Q2).
    
- _Overfitting:_ Using too high a polynomial degree can overfit – we saw an example where beyond degree 10, the model started to get worse on new data. We mitigate that by cross-validation or regularization (answering Q3).
    

I’d love to hear if you have any remaining curiosities or need clarification on any part of the talk. Feel free to ask about the math details, the HPC results, or how this might apply elsewhere (like could we use it for energy efficiency modeling, etc.?).

Thank you for your attention – I’m looking forward to your questions and thoughts!

## Slide 13: Test Questions (Post-Talk)

1. **Formulation:** How do we express polynomial regression as a linear system $A\mathbf{x}=\mathbf{b}$, and what methods (LU, QR, SVD) can be used to solve for the coefficients?
    
2. **HPC Motivation:** Why are surrogate models (like polynomial fits) useful for HPC performance tuning compared to exhaustive search or purely empirical trial-and-error?
    
3. **Model Complexity:** What happens if we choose a polynomial degree that’s too high for our data? How can we detect and prevent this overfitting issue in practice?
    

**Speaker Notes:**  
_(Revisiting the initial questions for reinforcement.)_ Now that we’ve gone through the talk, you should be comfortable answering these questions:

1. We set up $A$ using the input data ($x$ values) raised to powers for each polynomial term, and $\mathbf{b}$ as the outputs. Solving $A\mathbf{x}=\mathbf{b}$ in the least-squares sense gives the polynomial coefficients. Methods like QR factorization​[nhigham.com](https://nhigham.com/2020/09/15/what-is-a-householder-matrix/#:~:text=Householder%20matrices%20for%20computational%20purposes,to%20construct%20the%20QR%20factorization) or SVD​[utminers.utep.edu](https://utminers.utep.edu/xzeng/2017spring_math5330/MATH_5330_Computational_Methods_of_Linear_Algebra_files/ln15.pdf#:~:text=with%20C%20%3D%20AAt,The%20technique%20finds%20U%202) are used for a stable solution.
    
2. In HPC, surrogate models allow us to **predict** performance outcomes without having to run every possible experiment. They dramatically reduce the search space and give insights into performance trends, saving time on expensive supercomputer runs.
    
3. If the polynomial degree is too high, the model may fit noise – yielding near-perfect accuracy on training data but poor predictions on new data (overfitting). We saw an example where after degree 10 the error on new points worsened​file-7rhtrwtsjd5w8c8gletf19. To prevent this, we use techniques like cross-validation to find the sweet spot for model complexity, or add regularization to penalize overly complex models. We ensure the chosen model generalizes well, not just memorizes the training set.