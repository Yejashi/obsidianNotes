
## General Workflow

### Step 1 â€” **Runtime instrumentation (Caliper, Thicket, etc.)**
Collect:
- Region-level wall time, GPU time, and PAPI counters
- Metadata (kernel name, function name, source line)
These tell me _â€œwhat ran and how long it took.â€_

### Step 2 â€” **Compiler optimization remarks**
These tell me _â€œwhat the compiler did to this code.â€_

### Step 3 â€” **Correlate by region**
Map Caliper regions â†” source lines â†” remark entries.  
That lets me ask:
> â€œThis region got faster from -O2â†’-O3; what new remarks appeared or disappeared?â€

### Step 4 â€” **Hypothesis building**
From that correlation i hypothesize:
- â€œVectorization remark appeared â†’ likely SIMD speedup.â€
- â€œLoop unrolled â†’ maybe higher register pressure.â€
- â€œNo remark change but runtime changed â†’ backend or hardware effect.â€
    

---

##  When i need to _go deeper_

Once a hypothesis looks interesting (â€œwhy did -O3 slow down this region?â€), i descend the stack:

| Layer                             | What i inspect                                                                                        | Purpose                                                        | Machine-readable options                                                  |
| --------------------------------- | ----------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------- |
| **LLVM IR diffs**                 | Output from `opt -S -print-changed` or `-fpass-plugin=PrintFunction`                                  | See structural change (loop deleted, inlined, etc.)            | Use `-print-changed-format=json` (LLVM â‰¥17) â€” gives structured JSON diffs |
| **Optimization pass trace**       | `-debug-pass-manager` logs                                                                            | See which passes actually ran and in what order                | Parse textual trace or use `opt --print-pipeline-passes`                  |
| **Machine code / scheduling**     | `llvm-mca`, `llvm-objdump -d`, `-fsave-optimization-record -fsave-optimization-record-passes=machine` | Verify instruction mix, register pressure                      | `llvm-mca` JSON output gives IPC, bottlenecks                             |
| **Hardware performance counters** | Caliper, HPCToolkit, perf, rocprof                                                                    | Validate whether hypothesis holds (cache misses, stalls, etc.) | All export CSV/JSON                                                       |
| **Debug metadata linkage**        | `llvm-dwarfdump`, DWARF line tables                                                                   | Re-link machine code to source for region correlation          | Dwarf4/5 line info in binary, can parse via `pyelftools`                  |

By connecting these layers via `DebugLoc` or DWARF line info, i can make the â€œwhyâ€ traceable.

---
## Making it machine-readable

LLVM already gives me structured outputs at multiple levels:

| Artifact                         | Flag / tool                                      | Format           | Notes                               |
| -------------------------------- | ------------------------------------------------ | ---------------- | ----------------------------------- |
| **Optimization remarks**         | `-fsave-optimization-record                      | json             | YAML/JSON                           |
| **Pass diffs**                   | `opt -print-changed-format=json`                 | JSON             | Gives before/after IR for each pass |
| **Machine performance analysis** | `llvm-mca -output=json`                          | JSON             | Static throughput/latency model     |
| **Caliper data**                 | `cali-query --format json`                       | JSON             | Region metrics                      |
| **Binary-level info**            | `llvm-objdump --source --demangle --disassemble` | text (parseable) | Map instructions back to lines      |
   
---

## . What this buys me

I get a _progressive analysis model_:

| Layer                       | Question answered                         |
| --------------------------- | ----------------------------------------- |
| **Caliper runtime data**    | â€œWhich region is slow?â€                   |
| **Remarks**                 | â€œWhat optimizations did LLVM apply here?â€ |
| **IR diff**                 | â€œHow did the code structure change?â€      |
| **Machine code / counters** | â€œDid that change actually help or hurt?â€  |
And i only pay the cost of deeper introspection for the regions that look suspicious or interesting.

***
## Example workflow


## ðŸ§© 1. Start: the high-level _observation_

You have **measured runtime** for your app or region â€” say:

| Opt level | Runtime (ms) |
| --------- | ------------ |
| -O0       | 4.8          |
| -O3       | 0.02         |

Your first goal isnâ€™t to explain every micro-optimization â€” itâ€™s to _narrow down what changed semantically_.

So you start from the **performance delta**:

> â€œThe region got ~200Ã— faster.â€

---

## ðŸ§  2. Correlate with **remarks**

Now bring in your optimization remarks:

```
Loop deleted because it is invariant [-Rpass=loop-delete]
'_ZL3foov' not inlined because of noinline
marked as tail call candidate
```

At this level, remarks act like _semantic breadcrumbs_.  
They let you hypothesize:

|Remark|Possible performance meaning|
|---|---|
|`loop deleted`|Entire loop computation constant-folded away â†’ huge static simplification.|
|`not inlined`|Function call remains â†’ minor, predictable overhead.|
|`tail call`|Call frame reused â†’ negligible extra cost.|

So your initial hypothesis is simple:

> â€œThe loop was deleted because the compiler proved its effect is constant, reducing the function to a constant return.â€

This explains _qualitatively_ why performance improved.

---

## âš™ï¸ 3. Verify by looking at the **optimized IR**

Now you want to confirm whether your hypothesis is true.

**Command:**

```bash
clang -O3 -emit-llvm -S source.c -o opt.ll
```

Inspect `foo()` in `opt.ll`.

Youâ€™ll likely see:

```llvm
define dso_local i32 @foo() {
entry:
  ret i32 10
}
```

So indeed:

- Loop and arithmetic are gone.
    
- Function just returns a constant.
    

This confirms _structural elimination_ â†’ zero runtime work.

---

## ðŸ§® 4. Reconnect to **pass history**

If you log the pipeline (e.g., with `-debug-pass-manager`), youâ€™ll see:

```
SimplifyCFGPass
SROAPass
InstCombinePass
IndVarSimplifyPass
LoopDeletionPass
SimplifyCFGPass
```

The reasoning chain is now explicit:

1. **SROA / InstCombine** â†’ exposed that all ops are pure integer math.
    
2. **IndVarSimplify** â†’ proved loop trip count fixed and body side-effect-free.
    
3. **LoopDeletion** â†’ removed the loop.
    
4. **SimplifyCFG** â†’ cleaned up dead code.
    

â†’ Therefore, â€œloop deleted because it is invariantâ€ isnâ€™t arbitrary â€” itâ€™s the _consequence_ of those transformations.

---

## ðŸ”¬ 5. If you want to quantify _impact_, go down another layer

You can now connect:

- **Before/after IR** (`-print-changed-format=json`)
    
- **Assembly** (`llvm-objdump -d`)
    
- **Runtime measurement (Caliper)**
    

and demonstrate:

|Level|Before|After|
|---|---|---|
|IR|10-iteration loop|`ret i32 10`|
|ASM|~30 instructions|1 `mov`, 1 `ret`|
|Runtime|~4.8 ms|~0.02 ms|

Now you can write the full explanation:

> â€œBetween -O0 and -O3, LLVM identified that the loop in `foo()` had a fixed trip count and side-effect-free body. Through scalar replacement and induction variable simplification, it proved the final value was constant. `LoopDeletionPass` eliminated the loop, reducing the function to a constant return. This removed all iteration overhead, explaining the 200Ã— speedup. The remarks corroborate this (`loop deleted`, `tail call candidate`).â€

---

## ðŸ§  6. General method â€” â€œTop-down explanation pipelineâ€

Hereâ€™s the repeatable workflow you can apply to any region:

|Step|Question|Artifact|Tool|
|---|---|---|---|
|1|What changed in runtime?|Performance metrics|Caliper, Thicket|
|2|What optimizations fired?|Optimization remarks|`-fsave-optimization-record`|
|3|What structural code change occurred?|IR diff|`opt -print-changed-format=json`|
|4|What caused it?|Pass trace|`-debug-pass-manager`|
|5|Did it help?|Runtime + hardware counters|Caliper, perf, HPCToolkit|

Each level either _confirms_ or _refines_ your hypothesis.

---

## âœ… 7. TL;DR â€” in your example

|Level|Evidence|Conclusion|
|---|---|---|
|Performance|Runtime collapsed|Something got constant-folded|
|Remarks|`loop deleted because it is invariant`|Compiler proved loop redundant|
|IR|`ret i32 10`|Loop fully replaced by constant|
|Pass sequence|SROA â†’ InstCombine â†’ IndVarSimplify â†’ LoopDeletion|Causal chain established|
|Assembly|1 `mov`, 1 `ret`|Minimal instruction count|
|Runtime|200Ã— faster|Optimization fully explains behavior|

---

So your **top-down reasoning** looks like:

> **Observation:** Function runs faster.  
> **Clue:** â€œLoop deleted because it is invariant.â€  
> **Verification:** Optimized IR shows constant return.  
> **Explanation:** Earlier passes made loop provably invariant; LoopDeletion removed it.  
> **Result:** Constant folding â†’ no runtime loop â†’ major speedup.

Thatâ€™s the ideal structured analysis loop (no pun intended) â€” remarks guide your _hypothesis_, IR and pass data provide the _mechanistic explanation_, and runtime metrics confirm the _effect_.